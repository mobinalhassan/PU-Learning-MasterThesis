{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import DataLoader\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl = DataLoader()\n",
    "X_train, y_train, X_test, y_test = dl.load_mnist()\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 binary labels in training set: [1 0 0 1 1 0 1 1 1 0]\n",
      "First 10 binary labels in test set: [1 0 1 0 0 1 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "def convert_labels_to_binary(labels):\n",
    "    return np.where(labels % 2 == 0, 0, 1)\n",
    "\n",
    "\n",
    "y_train_binary = convert_labels_to_binary(y_train)\n",
    "y_test_binary = convert_labels_to_binary(y_test)\n",
    "\n",
    "print(\"First 10 binary labels in training set:\", y_train_binary[:10])\n",
    "print(\"First 10 binary labels in test set:\", y_test_binary[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FH-Deg\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the policy network\n",
    "def build_policy_network():\n",
    "    model = Sequential([\n",
    "        Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(2, activation='softmax')  # Outputs the probability of being positive or negative\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Define the classifier\n",
    "def build_classifier():\n",
    "    model = Sequential([\n",
    "        Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(1, activation='sigmoid')  # Binary classifier\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "policy_network = build_policy_network()\n",
    "classifier = build_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.9649 - loss: 0.1009\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m     classifier\u001b[38;5;241m.\u001b[39mfit(X_train, y_train_binary, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;66;03m# Update the policy network based on the classifier's performance\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m     \u001b[43mtrain_policy_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_binary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Test the classifier\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[16], line 33\u001b[0m, in \u001b[0;36mtrain_policy_network\u001b[1;34m(policy_network, classifier, X_train, y_train_binary)\u001b[0m\n\u001b[0;32m     30\u001b[0m gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(classifier_loss, policy_network\u001b[38;5;241m.\u001b[39mtrainable_variables)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Apply gradients to update the policy network\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgradients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:269\u001b[0m, in \u001b[0;36mBaseOptimizer.apply_gradients\u001b[1;34m(self, grads_and_vars)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_gradients\u001b[39m(\u001b[38;5;28mself\u001b[39m, grads_and_vars):\n\u001b[0;32m    268\u001b[0m     grads, trainable_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mgrads_and_vars)\n\u001b[1;32m--> 269\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;66;03m# Return iterations for compat with tf.keras.\u001b[39;00m\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterations\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:314\u001b[0m, in \u001b[0;36mBaseOptimizer.apply\u001b[1;34m(self, grads, trainable_variables)\u001b[0m\n\u001b[0;32m    310\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_variables_are_known(trainable_variables)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, caller\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    313\u001b[0m     \u001b[38;5;66;03m# Filter empty gradients.\u001b[39;00m\n\u001b[1;32m--> 314\u001b[0m     grads, trainable_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_filter_empty_gradients\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_variables\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(grads)) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:571\u001b[0m, in \u001b[0;36mBaseOptimizer._filter_empty_gradients\u001b[1;34m(self, grads, vars)\u001b[0m\n\u001b[0;32m    567\u001b[0m filtered \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    568\u001b[0m     (g, v) \u001b[38;5;28;01mfor\u001b[39;00m g, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(grads, \u001b[38;5;28mvars\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m g \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    569\u001b[0m ]\n\u001b[0;32m    570\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filtered:\n\u001b[1;32m--> 571\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo gradients provided for any variable.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(filtered) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(grads):\n\u001b[0;32m    573\u001b[0m     missing_grad_vars \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    574\u001b[0m         v \u001b[38;5;28;01mfor\u001b[39;00m g, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(grads, \u001b[38;5;28mvars\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m g \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    575\u001b[0m     ]\n",
      "\u001b[1;31mValueError\u001b[0m: No gradients provided for any variable."
     ]
    }
   ],
   "source": [
    "# Compile the models\n",
    "policy_network.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "def train_policy_network(policy_network, classifier, X_train, y_train_binary):\n",
    "    # Parameters\n",
    "    learning_rate = 1e-3\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # 1. Use the policy network to predict actions (pseudo-labels) for the training data\n",
    "        actions = policy_network(X_train, training=True)\n",
    "\n",
    "        # 2. Convert actions to pseudo-labels. This might involve taking the argmax if your actions are categorical.\n",
    "        pseudo_labels = tf.argmax(actions, axis=1)\n",
    "\n",
    "        # 3. Combine true labels and pseudo-labels\n",
    "        # For simplicity, we're treating all data as unlabeled here; you might have a more sophisticated method\n",
    "        combined_labels = tf.where(y_train_binary >= 0, y_train_binary, pseudo_labels)\n",
    "\n",
    "        # 4. Train the classifier on the combined dataset and calculate loss\n",
    "        classifier_predictions = classifier(X_train, training=True)\n",
    "        # Squeeze the predictions to match the target's shape\n",
    "        classifier_predictions = tf.squeeze(classifier_predictions, axis=-1)\n",
    "        classifier_loss = tf.keras.losses.binary_crossentropy(combined_labels, classifier_predictions)\n",
    "\n",
    "    # Calculate gradients with respect to policy network's parameters\n",
    "    gradients = tape.gradient(classifier_loss, policy_network.trainable_variables)\n",
    "\n",
    "    # Apply gradients to update the policy network\n",
    "    optimizer.apply_gradients(zip(gradients, policy_network.trainable_variables))\n",
    "\n",
    "\n",
    "# Training loop\n",
    "epochs = 10  # Example number of epochs\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Train the classifier on the original labeled data\n",
    "    classifier.fit(X_train, y_train_binary, epochs=1, batch_size=32, verbose=1)\n",
    "    \n",
    "    # Update the policy network based on the classifier's performance\n",
    "    train_policy_network(policy_network, classifier, X_train, y_train_binary)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{epochs} completed.')\n",
    "\n",
    "# Test the classifier\n",
    "test_loss, test_acc = classifier.evaluate(X_test, y_test_binary, verbose=2)\n",
    "print(f'\\nTest accuracy: {test_acc}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
